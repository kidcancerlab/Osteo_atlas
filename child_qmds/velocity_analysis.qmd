We are interested in how cell types and states in each sample change are evolving based on splicing dynamics. For this purpose will perform velocity analysis.

## Get .bam Files for Velocity Analysis

Since we have external data for this project not all .bam files are in the same location. As a result, we will have to conditionally create bam paths.

```{r get-bam-paths}
#making a list of bam paths, where each entry corresponds to an object and is a
#vector of all bams associated with that object

bam_list <- list()
obj_list <- qs::qread("output/matt_obj_list.qs")
obj_names <- names(obj_list)

for (obj in obj_names) {
    tmp_ob <- obj_list[[obj]] #nolint

    obj_list[[obj]] <- tmp_ob

    #get table of sample_names and the data source
    s_ids <- table("sample_name" = tmp_ob$sample_name,
                   "data_source" = tmp_ob$data_source) %>%
        as.data.frame() %>%
        subset(., Freq > 0) %>%
        select(1:2)

    ext_data_path <- "/home/gdrobertslab/lab/ExternalData/"

    bams <-
        ifelse(
               s_ids$data_source %in% c("NCH", "GEO"),
               paste0("/home/gdrobertslab/lab/Counts_2/",
                      s_ids$sample_name,
                      "/possorted_genome_bam.bam"),
               ifelse(
                      s_ids$data_source == "NCI_POB",
                      paste0(ext_data_path,
                             "McEachron_lab/BAMs/",
                             s_ids$sample_name,
                             "_gex_possorted_bam.bam"),
                      paste0(ext_data_path,
                             "Patel_lab/",
                             s_ids$sample_name,
                             "/possorted_genome_bam.bam")
                             )
                             )
    
    #fix paths for atac runs since bams have gex_ before possorted
    bams <- ifelse(file.exists(bams),
                   bams,
                   gsub("possorted_genome", "gex_possorted", bams))
    names(bams) <- s_ids$sample_name

    bam_list[[obj]] <- bams
}

#saving this list so that changing assignments won't mess up my analysis
qs::qsave(obj_list, "output/matt_obj_list.qs")

lapply(bam_list, file.exists)
```

## Make Loom Files

Now that we have our bams associated with each object we can make our loom files.

```{r make-loom-files}
lapply(names(obj_list), function(ob_name) {
    ob <- obj_list[[ob_name]]
    r_make_loom_files(sobj = ob,
                      id_col = "sample_name",
                      out_dir = "loom_output",
                      species = unique(ob$organism),
                      bam_paths = bam_list[[ob_name]],
                      cluster_account = "gdrobertslab",
                      slurm_base = "output/velocity_analysis/slurmOut",
                      sbatch_base = "output/velocity_analysis/sbatch_",
                      sobj_name = ob_name
                      )
    }
)
```

### Figure out Which Samples Failed

Not all samples had loom files successfully created, so I'm going to identify these and rerun `r_make_loom_files`.

Some more remained after this, so I'm running once more before digging deeper into this issue

```{r re-make-looms}
obj_list <- qs::qread("output/matt_obj_list.qs")

failed_runs <- lapply(names(obj_list), function(ob_name) {
    sids <- obj_list[[ob_name]]$sample_name %>% unique()
    failed <- sids[!file.exists(paste0("loom_output/",
                       ob_name,
                       "/loom_files/",
                       sids,
                       ".loom"))]
    if (length(failed) > 0) return(failed)
})
names(failed_runs) <- names(obj_list)

non_null <- list()
for (id in names(failed_runs)) {
    if (!is.null(failed_runs[[id]])) non_null[[id]] <- failed_runs[[id]]
}

lapply(names(non_null), function(ob_name) {
    ob <- obj_list[[ob_name]] %>%
        subset(sample_name %in% non_null[[ob_name]])
    r_make_loom_files(sobj = ob,
                      id_col = "sample_name",
                      out_dir = "loom_output",
                      species = unique(ob$organism),
                      bam_paths = bam_list[[ob_name]],
                      cluster_account = "gdrobertslab",
                      slurm_base = "output/velocity_analysis/slurmOut",
                      sbatch_base = "output/velocity_analysis/sbatch_",
                      sobj_name = ob_name)
})
```

## Add Metadata Columns to Objects

### Cell Cycle

I'm going to add cell cycle phases as a metadata feature to ensure that cell cycle effects aren't contributing to RNA velocity.

```{r add-cc}
obj_list <- lapply(obj_list, function(x) {
    CellCycleScoring(x,
                     s.features = cc.genes$s.genes,
                     g2m.features = cc.genes$g2m.genes)
})
```

### FDL

```{r add-fdl}
obj_list <- lapply(obj_list, run_fdl)
qs::qsave(obj_list, "output/matt_obj_list.qs")
```

## Write off Metadata for Velocity Analysis

```{r write-off-md}
lapply(names(obj_list), function(obj_name) {
    write_off_md(sobj = obj_list[[obj_name]],
                 id_col = "sample_name",
                 output_dir = paste0("loom_output/",
                                     obj_name,
                                     "/metadata"),
                 vars_to_keep = c("sample_name",
                                  "seurat_clusters",
                                  "Ann_level1",
                                  "Ann_level2",
                                  "Ann_level3",
                                  "Phase"))
})
```

## Analyze .loom Files

First I need to activate my conda environment. I set `eval = FALSE` because the environment path will change based on who is rendering this file.

```{bash activate-env, eval = FALSE}
conda activate /home/gdrobertslab/mjg015/R/x86_64-pc-linux-gnu-library/4.3/rrrSingleCellUtils/r_rna_velo
```

Next I need to import my conda libraries.

```{python load-py-libs}
import anndata
import scvelo as scv
import pandas as pd
import numpy as np
import matplotlib as plt
import scanpy as sc
import os
import re
```

Lastly I'm going to define a function that will reduce the amount of code needed
to create an anndata object from the loom file and metadata

```{python loom_to_ann}
def loom_to_an(loom_path, metadata_path):
    loom_files=os.listdir(loom_path)
    #initialize list to hold anndata objects
    ad_obs = []
    for one_loom in loom_files:
        #get sample id from file name
        sample_id = re.sub(".loom", "", one_loom)
        #add object to list
        tmp = anndata.read_loom(loom_path + "/" + one_loom)
        #read in metadata file
        md_path = metadata_path + "/" + sample_id + "_metadata.csv"
        metadata = pd.read_csv(md_path, index_col = "bc")
        tmp.obs = metadata.filter(regex = '^(?!umap_|PC_|fdl_|harmony_).*$').reindex(tmp.obs.index)
        #Add pca embeddings
        tmp.obsm["X_pca"] = metadata.filter(regex = "PC_").reindex(tmp.obs.index).to_numpy()
        #Add umap embeddings
        tmp.obsm["X_umap"] = metadata.filter(regex = "umap_").reindex(tmp.obs.index).to_numpy()
        #add fdl embeddings
        tmp.obsm["X_fdl"] = metadata.filter(regex = "fdl_").reindex(tmp.obs.index).to_numpy()
        #add harmony embeddings
        tmp.obsm["X_harmony"] = metadata.filter(regex = "harmony_").reindex(tmp.obs.index).to_numpy ()
        #make var names unique (this step is necessary to merge anndata list)
        tmp.var_names_make_unique()
        #add object to list
        ad_obs.append(tmp)
    #merge anndata list
    merged_anndata = anndata.concat(ad_obs)
    #return merged anndata
    return merged_anndata


```

### mm_mets_cancer_cells

```{python analyze-mm-mets-cancer-cells}
#get names of loom files
merged_ad = loom_to_an(loom_path = "loom_output/mm_mets_cancer_cells/loom_files",
                       metadata_path = "loom_output/mm_mets_cancer_cells/metadata")
```